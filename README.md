# Fluency Lab

> AI-powered communication coaching platform that helps users practice and improve their speaking skills through data-driven feedback.

Fluency Lab is a comprehensive web application designed to help users improve their communication skills through three main practice modes: **Job Interview**, **Elevator Pitch**, and **Presentation**. The platform records user video and audio, transcribes speech, analyzes communication metrics, and provides actionable feedback using advanced AI analysis.

## ğŸ¯ Inspiration

It is often challenging to judge yourself when preparing for an important presentation, interview, or pitch. Practicing alone rarely provides objective or actionable feedback, especially on delivery, pacing, and confidence. Fluency Lab addresses this by providing clear, data-driven insights into speaking habits, helping users improve their communication skills with each practice session.

## âœ¨ Features

### Practice Modes

- **Job Interview Mode**: Practice answering 1-5 behavioral interview questions with detailed feedback on clarity, pacing, structure, and confidence
- **Elevator Pitch Mode**: Master concise introductions under strict time constraints (60 seconds) with constructive feedback
- **Presentation Mode**: Upload slides or scripts and receive feedback on delivery, engagement, and overall presentation effectiveness

### Core Capabilities

- **Video & Audio Recording**: Integrated webcam interface for seamless recording
- **Speech Transcription**: Automatic speech-to-text using ElevenLabs API
- **Communication Analytics**: Extract metrics including:
  - Speaking rate and pacing
  - Pause patterns
  - Filler word usage
  - Fluency indicators
- **AI-Powered Feedback**: Google Gemini analyzes transcripts and metrics to generate structured, actionable feedback on:
  - Tone
  - Fluency
  - Vocabulary
  - Pronunciation
  - Engagement
  - Confidence
- **Progress Tracking**: Dashboard with session history and performance trends
- **User Authentication**: Secure JWT-based authentication system

## ğŸ› ï¸ Tech Stack

### Frontend
- **Next.js 14** - React framework with App Router
- **TypeScript** - Type-safe development
- **Tailwind CSS** - Utility-first styling
- **Radix UI** - Accessible component primitives
- **Recharts** - Data visualization for progress tracking
- **Lucide React** - Icon library

### Backend
- **Next.js API Routes** - Serverless API endpoints
- **MongoDB** - Database for users, recordings, transcriptions, and feedback
- **JWT** - Authentication tokens
- **bcryptjs** - Password hashing

### AI & Processing
- **Google Gemini API** - AI-powered feedback generation and question generation
- **ElevenLabs API** - Speech-to-text transcription
- **FFmpeg** - Video/audio processing (via VM service for production)

### Infrastructure
- **AWS S3** - Video and document storage
- **Vercel** - Deployment platform (with VM service for FFmpeg operations)

## ğŸ“‹ Prerequisites

- Node.js 18+ and npm
- MongoDB database (local or MongoDB Atlas)
- AWS S3 bucket (for production/storage)
- API keys:
  - Google Gemini API key
  - ElevenLabs API key
- (Optional) VM with FFmpeg for production deployment

## ğŸš€ Installation

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd DeltaHacks12
   ```

2. **Install dependencies**
   ```bash
   npm install
   ```

3. **Set up environment variables**
   
   Create a `.env.local` file in the root directory:
   ```env
   # Database
   MONGODB_URI=mongodb://localhost:27017/esl-coaching
   # or for MongoDB Atlas:
   # MONGODB_URI=mongodb+srv://username:password@cluster.mongodb.net/esl-coaching

   # Authentication
   NEXTAUTH_SECRET=your-secret-key-here

   # AI Services
   GOOGLE_GEMINI_API_KEY=your-gemini-api-key
   ELEVENLABS_API_KEY=your-elevenlabs-api-key

   # AWS S3 (required for production)
   AWS_REGION=us-east-1
   AWS_ACCESS_KEY_ID=your-access-key
   AWS_SECRET_ACCESS_KEY=your-secret-key
   AWS_S3_BUCKET=your-bucket-name

   # FFmpeg VM Service (optional, for production)
   FFMPEG_VM_URL=http://your-vm-url:3001
   FFMPEG_API_KEY=your-ffmpeg-api-key
   ```

4. **Run the development server**
   ```bash
   npm run dev
   ```

5. **Open your browser**
   Navigate to [http://localhost:3000](http://localhost:3000)

## ğŸ“ Project Structure

```
DeltaHacks12/
â”œâ”€â”€ app/                      # Next.js App Router
â”‚   â”œâ”€â”€ (auth)/              # Authentication routes
â”‚   â”‚   â”œâ”€â”€ login/
â”‚   â”‚   â””â”€â”€ register/
â”‚   â”œâ”€â”€ api/                 # API routes
â”‚   â”‚   â”œâ”€â”€ auth/           # Authentication endpoints
â”‚   â”‚   â”œâ”€â”€ feedback/       # Feedback generation
â”‚   â”‚   â”œâ”€â”€ gemini/         # Gemini AI integration
â”‚   â”‚   â”œâ”€â”€ process/        # Video processing
â”‚   â”‚   â”œâ”€â”€ recordings/     # Recording management
â”‚   â”‚   â”œâ”€â”€ transcriptions/ # Transcription endpoints
â”‚   â”‚   â”œâ”€â”€ upload/         # Video upload
â”‚   â”‚   â”œâ”€â”€ upload-reference/ # Document upload
â”‚   â”‚   â”œâ”€â”€ videos/         # Video serving
â”‚   â”‚   â””â”€â”€ whisper/        # Transcription trigger
â”‚   â”œâ”€â”€ dashboard/          # User dashboard
â”‚   â”œâ”€â”€ feedback/           # Feedback viewing
â”‚   â”œâ”€â”€ practice/           # Practice modes
â”‚   â”‚   â”œâ”€â”€ job-interview/
â”‚   â”‚   â”œâ”€â”€ elevator-pitch/
â”‚   â”‚   â””â”€â”€ presentation/
â”‚   â””â”€â”€ page.tsx            # Landing page
â”œâ”€â”€ components/              # React components
â”‚   â”œâ”€â”€ feedback/           # Feedback display components
â”‚   â”œâ”€â”€ recording/          # Video recording component
â”‚   â””â”€â”€ ui/                 # UI components
â”œâ”€â”€ lib/                     # Utility libraries
â”‚   â”œâ”€â”€ auth.ts             # Authentication utilities
â”‚   â”œâ”€â”€ db/                 # Database connection
â”‚   â”œâ”€â”€ gemini/             # Gemini AI integration
â”‚   â”œâ”€â”€ elevenlabs/         # ElevenLabs transcription
â”‚   â”œâ”€â”€ s3/                 # AWS S3 integration
â”‚   â””â”€â”€ vm-ffmpeg/          # FFmpeg VM service client
â”œâ”€â”€ types/                   # TypeScript type definitions
â”œâ”€â”€ public/                  # Static assets
â””â”€â”€ vm-ffmpeg-service/       # FFmpeg microservice (for production)
```

## ğŸ”§ Configuration

### Environment Variables

| Variable | Description | Required |
|----------|-------------|----------|
| `MONGODB_URI` | MongoDB connection string | Yes |
| `NEXTAUTH_SECRET` | Secret key for JWT tokens | Yes |
| `GOOGLE_GEMINI_API_KEY` | Google Gemini API key | Yes |
| `ELEVENLABS_API_KEY` | ElevenLabs API key | Yes |
| `AWS_REGION` | AWS region for S3 | Production |
| `AWS_ACCESS_KEY_ID` | AWS access key | Production |
| `AWS_SECRET_ACCESS_KEY` | AWS secret key | Production |
| `AWS_S3_BUCKET` | S3 bucket name | Production |
| `FFMPEG_VM_URL` | FFmpeg service URL | Production |
| `FFMPEG_API_KEY` | FFmpeg service API key | Production |

### FFmpeg VM Service (Production)

For production deployments on Vercel, video processing is handled by a separate VM service to avoid timeout limitations. See `vm-ffmpeg-service/README.md` for setup instructions.

## ğŸ® Usage

1. **Register/Login**: Create an account or log in to access the platform
2. **Choose Practice Mode**: Select from Job Interview, Elevator Pitch, or Presentation
3. **Record**: Use your webcam to record your practice session
4. **Review Feedback**: Get detailed AI-powered feedback on your performance
5. **Track Progress**: View your improvement over time on the dashboard

### Job Interview Mode
- Select number of questions (1-5)
- Answer behavioral interview questions
- Receive feedback on clarity, structure, and confidence

### Elevator Pitch Mode
- Practice a 60-second introduction
- Get feedback on conciseness and impact

### Presentation Mode
- Upload slides or script (optional)
- Deliver your presentation
- Receive comprehensive feedback on delivery and engagement

## ğŸ—ï¸ Architecture

### Data Flow

1. **Recording**: User records video/audio via webcam
2. **Upload**: Video uploaded to S3 (or local storage in dev)
3. **Transcription**: Audio extracted and transcribed via ElevenLabs
4. **Analysis**: Transcription and metrics analyzed by Google Gemini
5. **Feedback**: Structured feedback generated and stored
6. **Display**: User views feedback with visualizations

### Database Collections

- `users` - User accounts and authentication
- `recordings` - Video recordings metadata
- `transcriptions` - Speech transcription data
- `feedbackReports` - AI-generated feedback reports
- `referenceDocuments` - Uploaded slides/scripts

## ğŸš¢ Deployment

### Vercel Deployment

1. Push code to GitHub
2. Import project in Vercel
3. Configure environment variables
4. Deploy

**Note**: For production, you'll need to set up the FFmpeg VM service separately. See `vm-ffmpeg-service/DEPLOYMENT.md` for details.

### Local Development

The app can run locally with:
- Local MongoDB instance
- Local file storage (no S3 required)
- FFmpeg installed locally (via `ffmpeg-static` package)

## ğŸ§ª Development

```bash
# Install dependencies
npm install

# Run development server
npm run dev

# Build for production
npm run build

# Start production server
npm start

# Lint code
npm run lint
```

## ğŸ¯ Challenges & Solutions

### Challenges Faced

- **Reliable Speech Metrics**: Extracting meaningful fluency metrics from speech data in real-time
- **Balanced Feedback**: Designing prompts that provide specific, encouraging feedback without being overly generic or critical
- **Feature Scope**: Balancing feature completeness with polish within hackathon timeframe
- **Video Processing**: Handling large video files and processing on serverless platforms

### Solutions Implemented

- **Robust Transcription Pipeline**: Using ElevenLabs for accurate transcription with word-level timestamps
- **Careful Prompt Engineering**: Structured prompts that generate balanced, actionable feedback
- **Modular Architecture**: Clean separation of concerns for maintainability
- **VM Service**: Separate FFmpeg service for production video processing

## ğŸ† Accomplishments

- âœ… Complete end-to-end system with immediate, actionable feedback
- âœ… Multiple practice modes (Interview, Elevator Pitch, Presentation)
- âœ… Data-driven insights from raw speech data
- âœ… Strong MVP delivered within hackathon timeline
- âœ… Polished user interface with progress tracking

## ğŸ“š What We Learned

- Value of combining speech analytics with LLMs for skill development
- Building reliable speech-processing pipelines
- Designing structured AI outputs for consistent feedback
- Creating user-focused feedback systems under time constraints
- Serverless architecture considerations for media processing

## ğŸ”® What's Next

Future enhancements planned:

- **Real-time Feedback**: Live feedback during practice sessions
- **Progress Tracking**: Long-term progress analytics and trends
- **Additional Scenarios**: Team meetings, impromptu speaking practice
- **Enhanced Scoring**: Refined fluency scoring algorithms
- **Visual Feedback**: Timeline visualizations for confidence/engagement
- **Personalization**: Improved personalization and adaptive learning

## ğŸ“ License

This project was created for DeltaHacks 12 hackathon.

## ğŸ‘¥ Contributors

Built with â¤ï¸ by the Fluency Lab team.

---

For questions or issues, please open an issue on the repository.
